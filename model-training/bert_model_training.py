# -*- coding: utf-8 -*-
"""last-model.ipynb

Automatically generated by Colab.

"""

!pip install transformers torch datasets


import pandas as pd
import tensorflow as tf
import torch
import numpy as np
import time
import datetime
import random
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

from google.colab import drive

!pip install transformers
import transformers
from transformers import BertTokenizer
from torch.utils.data import TensorDataset, random_split
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
from transformers import BertForSequenceClassification, BertConfig
from transformers import get_linear_schedule_with_warmup

from torch.optim import AdamW

from sklearn.metrics import f1_score, recall_score, precision_score, classification_report
from sklearn.preprocessing import LabelEncoder

df = pd.read_csv(
    "last-sms.csv",
    encoding="utf-8-sig",
    sep=";",
    engine="python"
)

df.columns = (
    df.columns
    .str.replace("ï»¿", "", regex=False)
    .str.strip()
)

df.head(20)

!pip install ftfy

# encoding ve bozuk karakter problemlerini düzelt
import ftfy

df["text"] = df["text"].astype(str).apply(ftfy.fix_text)
df["groupText"] = df["groupText"].astype(str).apply(ftfy.fix_text)
df["masked_text"] = df["masked_text"].astype(str).apply(ftfy.fix_text)

print("\nSONRA:")
print(df.iloc[17]["text"])

print(df["groupText"].value_counts())

from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(
    df,
    test_size=0.2,
    random_state=42,
    stratify=df["group"]
)

train_df, val_df = train_test_split(
    train_df,
    test_size=0.2,
    random_state=42,
    stratify=train_df["group"]
)

print(train_df["groupText"].value_counts())

class_counts = {
    "Dolandiricilik": 429,
    "Promosyon": 576,
    "Normal": 527
}

max_count = max(class_counts.values())

# sınıf dengesizliğini azaltmak için  class weight 

class_weights = torch.tensor(
    [
        max_count / class_counts["Dolandiricilik"],
        max_count / class_counts["Promosyon"],
        max_count / class_counts["Normal"]
    ],
    dtype=torch.float
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
class_weights = class_weights.to(device)

loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)

print("Class weights:", class_weights)

#data leakage var mı
len(set(train_df["masked_text"]).intersection(set(test_df["masked_text"])))

train_texts = train_df["masked_text"].values
val_texts   = val_df["masked_text"].values
test_texts  = test_df["masked_text"].values

tokenizer = BertTokenizer.from_pretrained(
    "dbmdz/bert-base-turkish-cased"
)

sample_text = train_df.loc[0, "masked_text"]

enc = tokenizer(
    sample_text,
    truncation=True,
    padding=False,
    max_length=128
)

print("ORİJİNAL METİN:")
print(sample_text)
print("\nTOKEN'LAR:")
print(tokenizer.convert_ids_to_tokens(enc["input_ids"]))
print("\nTOKEN ID'LER:")
print(enc["input_ids"])

label2id = {
    "Dolandiricilik": 0,
    "Promosyon": 1,
     "Normal": 2

}

# metinleri tokenizer ile  input_ids ve attention_mask e dönüştür
def encode_texts(text_list, tokenizer, max_len):
    encoded = tokenizer(
        list(text_list),
        add_special_tokens=True,
        max_length=max_len,
        padding="max_length",
        truncation=True,
        return_attention_mask=True,
        return_tensors="pt"
    )
    return encoded["input_ids"], encoded["attention_mask"]

max_len = 128

# train, validation ve test verilerini tokenize et ve etiketleri sayısal tensörlere dönüştür
train_input_ids, train_attention_masks = encode_texts(
    train_df["masked_text"], tokenizer, max_len
)

val_input_ids, val_attention_masks = encode_texts(
    val_df["masked_text"], tokenizer, max_len
)

test_input_ids, test_attention_masks = encode_texts(
    test_df["masked_text"], tokenizer, max_len
)


train_labels = torch.tensor(
    train_df["groupText"].map(label2id).values,
    dtype=torch.long
)

val_labels = torch.tensor(
    val_df["groupText"].map(label2id).values,
    dtype=torch.long
)

test_labels = torch.tensor(
    test_df["groupText"].map(label2id).values,
    dtype=torch.long
)

print(train_input_ids.shape)
print(train_attention_masks.shape)
print(train_labels.shape)

print(torch.unique(train_labels))

print(torch.unique(train_labels))
print(train_input_ids.shape, train_attention_masks.shape)

#TensorDataset ve DataLoader yapıları
from torch.utils.data import DataLoader, TensorDataset

batch_size = 32

train_dataset = TensorDataset(
    train_input_ids,
    train_attention_masks,
    train_labels
)

val_dataset = TensorDataset(
    val_input_ids,
    val_attention_masks,
    val_labels
)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

from transformers import BertForSequenceClassification
from torch.optim import AdamW

#modeli yukledık  ve optimizer tanımladık
model = BertForSequenceClassification.from_pretrained(
    "dbmdz/bert-base-turkish-cased",
    num_labels=3
)

model.to(device)

optimizer = AdamW(model.parameters(), lr=3e-5)

# learning rate i warmup ve linear decay ile dinamik olarak ayarladık
from transformers import get_linear_schedule_with_warmup

total_steps = len(train_loader) * epochs

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=int(0.1 * total_steps),
    num_training_steps=total_steps
)


#model egitimi
epochs = 3

for epoch in range(epochs):
    print(f"\nEpoch {epoch+1}/{epochs}")

    model.train()
    total_train_loss = 0

    for batch in train_loader:
        batch_input_ids, batch_attention_masks, batch_labels = [
            b.to(device) for b in batch
        ]

        optimizer.zero_grad()

        outputs = model(
            input_ids=batch_input_ids,
            attention_mask=batch_attention_masks
        )

        logits = outputs.logits
        loss = loss_fn(logits, batch_labels)

        loss.backward()
        optimizer.step()
        scheduler.step()

        total_train_loss += loss.item()

    avg_train_loss = total_train_loss / len(train_loader)
    print("Train Loss:", avg_train_loss)


    model.eval()
    total_val_loss = 0

    with torch.no_grad():
        for batch in val_loader:
            batch_input_ids, batch_attention_masks, batch_labels = [
                b.to(device) for b in batch
            ]

            outputs = model(
                input_ids=batch_input_ids,
                attention_mask=batch_attention_masks
            )

            logits = outputs.logits
            loss = loss_fn(logits, batch_labels)

            total_val_loss += loss.item()

    avg_val_loss = total_val_loss / len(val_loader)
    print("Val Loss:", avg_val_loss)

total = len(train_df)
unique = train_df["masked_text"].nunique()

print("Toplam satır:", total)
print("Benzersiz mesaj:", unique)
print("Tekrar sayısı:", total - unique)


#model degerlendirme
model.eval()

all_preds = []
all_labels = []

with torch.no_grad():
    for batch in val_loader:
        batch_input_ids, batch_attention_masks, batch_labels = [
            b.to(device) for b in batch
        ]

        outputs = model(
            input_ids=batch_input_ids,
            attention_mask=batch_attention_masks
        )

        preds = torch.argmax(outputs.logits, dim=1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(batch_labels.cpu().numpy())

all_preds = np.array(all_preds)
all_labels = np.array(all_labels)

class_names = ["Dolandiricilik", "Promosyon", "Normal"]

print("\nCLASSIFICATION REPORT (Precision / Recall / F1)\n")
print(
    classification_report(
        all_labels,
        all_preds,
        target_names=class_names,
        digits=4
    )
)

#confusion matris
from sklearn.metrics import confusion_matrix

class_names = ["Dolandırıcılık", "Promosyon", "Normal"]


cm = confusion_matrix(all_labels, all_preds)

plt.figure(figsize=(6, 5))
sns.heatmap(
    cm,
    annot=True,
    fmt="d",
    cmap="Blues",
    xticklabels=class_names,
    yticklabels=class_names
)

plt.xlabel("Tahmin Edilen Sınıf")
plt.ylabel("Gerçek Sınıf")
plt.title("Confusion Matrix")
plt.tight_layout()
plt.show()
